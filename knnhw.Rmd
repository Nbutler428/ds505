---
title: $K$NN
author: "Nathan Butler"
date: "02/10/2025"

format: 
  html:  # You will quite likely want to change all but the last one, to taste
    theme: superhero  
    mainfont: monospace
    highlight-style: github
    title-block-banner: true
    embed-resources: true

---

**Abstract:**

This is a technical blog post of **both** an HTML file *and* [.qmd file](https://raw.githubusercontent.com/cd-public/D505/refs/heads/master/hws/src/knn.qmd) hosted on GitHub pages.

# 0. Quarto Type-setting

- This document is rendered with Quarto, and configured to embed an images using the `embed-resources` option in the header.
- If you wish to use a similar header, here's is the format specification for this document:

```email
format: 
  html:
    embed-resources: true
```

# 1. Setup

```{r}
library(tidyverse)
library(caret)
wine <- readRDS(gzcon(url("https://github.com/cd-public/D505/raw/master/dat/pinot.rds")))
```

## 2. $K$NN Concepts

> <span style="color:red;font-weight:bold">TODO</span>: *Explain how the choice of K affects the quality of your prediction when using a $K$ Nearest Neighbors algorithm.*

**Explanation:The choice of K in kNN impacts how the model generalizes. A small K (e.g., 1 or 3) makes predictions sensitive to noise, leading to overfitting. A large K smooths predictions by considering more neighbors but risks under fitting by ignoring local patterns. The best K balances bias and variance. You can usually find this balance by doing some cross validation.**

## 3. Feature Engineering

1. Create a version of the year column that is a *factor* (instead of numeric).
2. Create dummy variables that indicate the presence of "cherry", "chocolate" and "earth" in the description.
  - Take care to handle upper and lower case characters.
3. Create 3 new features that represent the interaction between *time* and the cherry, chocolate and earth indicators.
4. Remove the description column from the data.

```{r}
wine2 <- wine %>% 
  mutate(fyear = as.factor(year), 
         cherry_dummy = as.integer(str_detect(tolower(description), "cherry")),
         earthy_dummy = as.integer(str_detect(tolower(description), "earth|earthy")),
         choc_dummy = as.integer(str_detect(tolower(description), "chocolate")),
         cherry_time = as.integer(year) * cherry_dummy,
         earth_time = as.integer(year) * earthy_dummy,
         choc_time = as.integer(year) * choc_dummy) %>% 
  select(-description)
```
## 4. Preprocessing

1. Preprocess the dataframe from the previous code block using BoxCox, centering and scaling of the numeric features
2. Create dummy variables for the `year` factor column
```{r}
pacman::p_load(fastDummies)
```


```{r}
wine2<- wine2 %>% 
  dummy_cols(select_columns = 'fyear', remove_selected_columns = T) %>% 
  select(-year)
```
```{r}
box_wine<-wine2 %>% 
  preProcess(method = c("BoxCox","center","scale")) %>% 
  predict(wine2)
head(box_wine)
```



## 5. Running $K$NN

1. Split the dataframe into an 80/20 training and test set
2. Use Caret to run a $K$NN model that uses our engineered features to predict province
  - use 5-fold cross validated subsampling 
  - allow Caret to try 15 different values for $K$
3. Display the confusion matrix on the test data


```{r}
set.seed(505)
wine_index <- createDataPartition(box_wine$province, p = 0.8, list = FALSE)
train <- box_wine[wine_index, ]
test <- box_wine[-wine_index, ]
```
```{r}
fit <- train(province ~ .,
             data = train, 
             method = "knn",
             tuneLength = 15,
             trControl = trainControl(number = 5))
```

```{r}
confusionMatrix(predict(fit, test), factor(test$province))
```

## 6. Kappa

How do we determine whether a Kappa value represents a good, bad or some other outcome?

> <span style="color:red;font-weight:bold">TODO</span>: *Kappa is used to assess whether a model's performance is meaningfully better than random chance. It does so by comparing the observed agreement (how often the model's predictions match actual values) to the expected agreement (the level of agreement we would expect by chance). While this is a simplified interpretation of the equation, a high Kappa value generally indicates strong model performance. In most cases, a Kappa value above 0.6 is considered good, as it suggests substantial agreement. However, it is important to be cautious if Kappa exceeds 0.85 or 0.9, as this may indicate overfitting, meaning the model is too closely tailored to the training data and may not generalize well to new data.*

## 7. Improvement

How can we interpret the confusion matrix, and how can we improve in our predictions?

> <span style="color:red;font-weight:bold">TODO</span>: *Explain:The confusion matrix indicates that the model performs well in classifying California wines but has difficulty distinguishing between Burgundy and Oregon, frequently misclassifying them. Additionally, Casablanca Valley and Marlborough are not classified at all, suggesting the model fails to recognize them. To enhance performance, potential improvements include balancing the dataset, incorporating more informative features, optimizing the k-value in kNN, or utilizing a more advanced model such as Random Forest.*